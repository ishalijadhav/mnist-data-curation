{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Introduction\n",
        "\n",
        "In this assignment, I build a complete pipeline for curating the MNIST dataset, training a baseline LeNet classifier, identifying uncertain samples using CLIP embeddings, and creating an improved classifier with an additional IDK (I Don’t Know) class.\n",
        "\n",
        "The goal is to simulate a real-world dataset curation workflow:\n",
        "\n",
        "Identify mislabeled or low-confidence samples\n",
        "\n",
        "Reassign them to an IDK class\n",
        "\n",
        "Retrain a classifier with 11 classes\n",
        "\n",
        "Compare performance between the original and improved models"
      ],
      "metadata": {
        "id": "b0MlDTr0Yra7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_N1XmSWQajd"
      },
      "outputs": [],
      "source": [
        "\n",
        "%%capture\n",
        "!uv pip install fiftyone==1.7.0 torch==2.6.0 torchvision==0.21 numpy==2.0.2 open-clip-torch==3.2.0\n",
        "!fiftyone plugins download https://github.com/voxel51/fiftyone-plugins --plugin-names @voxel51/evaluation\n",
        "!fiftyone plugins download https://github.com/jacobmarks/fiftyone-albumentations-plugin"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Setup & Dependencies"
      ],
      "metadata": {
        "id": "90KfIatbYzlE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4AX1d65QbKI"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as Fun\n",
        "import torchvision.transforms.v2 as transforms\n",
        "import fiftyone as fo\n",
        "import fiftyone.zoo as foz\n",
        "import fiftyone.brain as fob\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set Seeds for Reproducibility\n",
        "def set_seeds(seed=51):\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "\n",
        "set_seeds()\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Loading the MNIST Dataset"
      ],
      "metadata": {
        "id": "rkiJvzbaY49U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTCag_lWnM6j"
      },
      "outputs": [],
      "source": [
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Visualizing Sample Images"
      ],
      "metadata": {
        "id": "vWhw664HY9Ft"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2kIiaqvQxTW"
      },
      "outputs": [],
      "source": [
        "# Modified version\n",
        "# Load MNIST Test split\n",
        "dataset = foz.load_zoo_dataset(\"mnist\", split=\"test\")\n",
        "\n",
        "# Load CLIP model\n",
        "clip_model = foz.load_zoo_model(\"clip-vit-base32-torch\", device=device)\n",
        "\n",
        "# 1. Compute Embeddings\n",
        "dataset.compute_embeddings(\n",
        "    model=clip_model,\n",
        "    embeddings_field=\"clip_embeddings\",\n",
        "    batch_size=512\n",
        ")\n",
        "\n",
        "# 2. Run PCA (Fast, linear)\n",
        "fob.compute_visualization(\n",
        "    dataset,\n",
        "    embeddings=\"clip_embeddings\",\n",
        "    method=\"pca\",\n",
        "    brain_key=\"pca_vis\"\n",
        ")\n",
        "\n",
        "# 3. Run UMAP (Slower, captures clusters better)\n",
        "fob.compute_visualization(\n",
        "    dataset,\n",
        "    embeddings=\"clip_embeddings\",\n",
        "    method=\"umap\",\n",
        "    brain_key=\"umap_vis\"\n",
        ")\n",
        "\n",
        "# Launch App to see it (Lab Requirement 1)\n",
        "session = fo.launch_app(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Training the First LeNet Classifier\n",
        "\n",
        "In this section:\n",
        "\n",
        "I implement a LeNet-style CNN\n",
        "\n",
        "Train it on the original 10 MNIST classes\n",
        "\n",
        "Evaluate accuracy\n",
        "\n",
        "Save the trained model weights\n",
        "\n",
        "This model is used to generate predictions and confidence values for dataset curation."
      ],
      "metadata": {
        "id": "JnYmkLZfZDtN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B89ACTNgQ5AK"
      },
      "outputs": [],
      "source": [
        "# Modified version\n",
        "class ModernLeNet5(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.conv3 = nn.Conv2d(16, 120, 4)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(120, 84)\n",
        "        self.fc2 = nn.Linear(84, num_classes)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(Fun.relu(self.conv1(x)))\n",
        "        x = self.pool(Fun.relu(self.conv2(x)))\n",
        "        x = Fun.relu(self.conv3(x))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = Fun.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        return self.fc2(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vL94qV8TGQD"
      },
      "outputs": [],
      "source": [
        "\n",
        "class FiftyOneTorchDataset(Dataset):\n",
        "    def __init__(self, fo_dataset, transforms=None):\n",
        "        self.samples = fo_dataset.values(\"filepath\")\n",
        "        self.labels = fo_dataset.values(\"ground_truth.label\")\n",
        "        self.transforms = transforms\n",
        "        # Map \"0 - zero\" to 0, etc.\n",
        "        self.label_map = {l: i for i, l in enumerate(sorted(fo_dataset.distinct(\"ground_truth.label\")))}\n",
        "\n",
        "    def __len__(self): return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.samples[idx]).convert(\"L\")\n",
        "        if self.transforms: img = self.transforms(img)\n",
        "        label_str = self.labels[idx]\n",
        "        return img, self.label_map.get(label_str, -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIgmYu2lTKAP"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load Training Data\n",
        "train_data = foz.load_zoo_dataset(\"mnist\", split=\"train\")\n",
        "\n",
        "# Transforms\n",
        "tfms = transforms.Compose([\n",
        "    transforms.ToImage(),\n",
        "    transforms.ToDtype(torch.float32, scale=True),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# Train Loop Setup\n",
        "model = ModernLeNet5().to(device)\n",
        "opt = Adam(model.parameters(), lr=0.001)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "loader = DataLoader(FiftyOneTorchDataset(train_data, tfms), batch_size=64, shuffle=True)\n",
        "\n",
        "# Train (1 Epoch is enough to find hard samples)\n",
        "print(\"Training baseline...\")\n",
        "model.train()\n",
        "for imgs, lbls in tqdm(loader):\n",
        "    imgs, lbls = imgs.to(device), lbls.to(device)\n",
        "    opt.zero_grad()\n",
        "    loss_fn(model(imgs), lbls).backward()\n",
        "    opt.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Generating Predictions & Confidence Scores\n",
        "\n",
        "Using the trained model, I compute:\n",
        "\n",
        "Logits for each training image\n",
        "\n",
        "Softmax probabilities\n",
        "\n",
        "Model confidence for predicted label\n",
        "\n",
        "These values are added back into the FiftyOne dataset for inspection."
      ],
      "metadata": {
        "id": "lIJOgg7tZNZo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hJEHLrhTM0q"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "# --- 1. Setup for Inference ---\n",
        "# Use the same dataset wrapper, but we turn off shuffling to keep labels aligned\n",
        "inference_dataset = FiftyOneTorchDataset(train_data, tfms)\n",
        "inference_loader = DataLoader(inference_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "# Get our class list (0-9) so we can map predictions back to strings\n",
        "classes = sorted(train_data.distinct(\"ground_truth.label\"))\n",
        "\n",
        "# --- 2. Run Inference (Manual Loop) ---\n",
        "print(\"Generating predictions manually...\")\n",
        "model.eval()\n",
        "predictions = []\n",
        "\n",
        "# Turn off gradients to save memory\n",
        "with torch.no_grad():\n",
        "    for imgs, _ in tqdm(inference_loader):\n",
        "        imgs = imgs.to(device)\n",
        "\n",
        "        # Get raw output (logits) from the model\n",
        "        logits = model(imgs)\n",
        "\n",
        "        # Convert to probabilities (confidence scores)\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "\n",
        "        # Move data back to CPU for processing\n",
        "        logits = logits.cpu().numpy()\n",
        "        probs = probs.cpu().numpy()\n",
        "\n",
        "        # Create a FiftyOne Classification object for every image\n",
        "        for i in range(len(logits)):\n",
        "            pred_idx = np.argmax(probs[i])\n",
        "\n",
        "            # We store the label, the confidence, AND the logits\n",
        "            # The logits are required for computing hardness/mistakenness\n",
        "            predictions.append(\n",
        "                fo.Classification(\n",
        "                    label=classes[pred_idx],\n",
        "                    confidence=probs[i][pred_idx],\n",
        "                    logits=logits[i].tolist()\n",
        "                )\n",
        "            )\n",
        "\n",
        "# --- 3. Save to Dataset ---\n",
        "print(\"Saving to FiftyOne dataset...\")\n",
        "# This bulk operation is much faster than saving samples one by one\n",
        "train_data.set_values(\"predictions\", predictions)\n",
        "\n",
        "# --- 4. Compute Hardness ---\n",
        "print(\"Computing hardness...\")\n",
        "fob.compute_hardness(train_data, label_field=\"predictions\")\n",
        "\n",
        "print(\"Hardness computation complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgYSeE1Shx1t"
      },
      "outputs": [],
      "source": [
        "# Modified version\n",
        "if \"mnist-curated-idk\" in fo.list_datasets():\n",
        "    fo.delete_dataset(\"mnist-curated-idk\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzPP_xBrU9lz"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 1. Clone the dataset so we don't ruin the original\n",
        "idk_dataset = train_data.clone()\n",
        "idk_dataset.name = \"mnist-curated-idk\"\n",
        "idk_dataset.persistent = True\n",
        "\n",
        "# 2. Find the hardest samples\n",
        "hardness_thresh = idk_dataset.quantiles(\"hardness\", [0.98])[0]\n",
        "questionable_view = idk_dataset.match(fo.ViewField(\"hardness\") > hardness_thresh)\n",
        "\n",
        "print(f\"Found {len(questionable_view)} questionable samples.\")\n",
        "\n",
        "# 3. Relabel them as '10 - IDK'\n",
        "for sample in questionable_view:\n",
        "    sample[\"ground_truth\"] = fo.Classification(label=\"10 - IDK\")\n",
        "    sample.save()\n",
        "\n",
        "# 4. Verify classes\n",
        "print(\"New Classes:\", idk_dataset.distinct(\"ground_truth.label\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3Y34sBAfq_-"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Add 'questionable' tag to these samples for visualization in FiftyOne\n",
        "for sample in questionable_view:\n",
        "    sample.tags.append(\"questionable\")\n",
        "    sample.save()\n",
        "\n",
        "print(\"Added 'questionable' tag to all hard samples.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Training the Second LeNet Classifier (11 Classes)\n",
        "\n",
        "Now I retrain LeNet, but this time:\n",
        "\n",
        "With 11 output neurons\n",
        "\n",
        "Using the IDK-augmented dataset"
      ],
      "metadata": {
        "id": "c9Gj9DNRZj-I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxddBt6KVrJJ"
      },
      "outputs": [],
      "source": [
        "# Modified version\n",
        "# 1. New Model with 11 Classes\n",
        "idk_model = ModernLeNet5(num_classes=11).to(device)\n",
        "opt = Adam(idk_model.parameters(), lr=0.001)\n",
        "\n",
        "# 2. New Dataset Wrapper (Auto-updates label map for 11 classes)\n",
        "idk_torch_data = FiftyOneTorchDataset(idk_dataset, tfms)\n",
        "idk_loader = DataLoader(idk_torch_data, batch_size=64, shuffle=True)\n",
        "\n",
        "# 3. Train Again\n",
        "print(\"Training IDK Classifier...\")\n",
        "idk_model.train()\n",
        "for epoch in range(3): # Train a bit longer this time\n",
        "    for imgs, lbls in tqdm(idk_loader):\n",
        "        imgs, lbls = imgs.to(device), lbls.to(device)\n",
        "        opt.zero_grad()\n",
        "        loss_fn(idk_model(imgs), lbls).backward()\n",
        "        opt.step()\n",
        "\n",
        "print(\"Training Complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eATajT0yV1RW"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- 1. Prepare Test Data for 11 Classes ---\n",
        "# We use the SAME label map from training so the model knows \"10\" means \"IDK\"\n",
        "test_dataset = foz.load_zoo_dataset(\"mnist\", split=\"test\")\n",
        "\n",
        "# Re-create the mapping (0-9 + IDK)\n",
        "idk_label_map = {l: i for i, l in enumerate(sorted(idk_dataset.distinct(\"ground_truth.label\")))}\n",
        "\n",
        "# Test Loader\n",
        "test_torch_data = FiftyOneTorchDataset(test_dataset, tfms) # Helper class we defined earlier\n",
        "test_loader = DataLoader(test_torch_data, batch_size=64, shuffle=False) # No shuffle for evaluation!\n",
        "\n",
        "# --- 2. Run Inference ---\n",
        "print(\"Evaluating on Test Set...\")\n",
        "idk_model.eval()\n",
        "idk_predictions = []\n",
        "\n",
        "# Get list of class names (e.g., \"0 - zero\", ..., \"10 - IDK\")\n",
        "# We sort by value to ensure index 10 corresponds to \"10 - IDK\"\n",
        "class_names = sorted(idk_label_map, key=idk_label_map.get)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, _ in tqdm(test_loader):\n",
        "        imgs = imgs.to(device)\n",
        "        logits = idk_model(imgs)\n",
        "        probs = Fun.softmax(logits, dim=1).cpu().numpy()\n",
        "\n",
        "        for i in range(len(probs)):\n",
        "            pred_idx = np.argmax(probs[i])\n",
        "            idk_predictions.append(\n",
        "                fo.Classification(\n",
        "                    label=class_names[pred_idx],\n",
        "                    confidence=probs[i][pred_idx]\n",
        "                )\n",
        "            )\n",
        "\n",
        "# --- 3. Store & Visualize ---\n",
        "test_dataset.set_values(\"idk_predictions\", idk_predictions)\n",
        "\n",
        "# Generate the report (Accuracy, Precision, etc.)\n",
        "results = test_dataset.evaluate_classifications(\n",
        "    \"idk_predictions\",\n",
        "    gt_field=\"ground_truth\",\n",
        "    eval_key=\"eval_idk\"\n",
        ")\n",
        "\n",
        "print(\"Evaluation Results:\")\n",
        "results.print_report()\n",
        "\n",
        "# Plot Confusion Matrix (Required for Lab!)\n",
        "plot = results.plot_confusion_matrix()\n",
        "plot.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6pmmOsoWlqO"
      },
      "outputs": [],
      "source": [
        "# Modified version\n",
        "session = fo.launch_app(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Saving the Curated Dataset & Predictions"
      ],
      "metadata": {
        "id": "Xe9FSeV-ZrwF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3wsdIy_kNwj"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBUbXpYkt_r-"
      },
      "outputs": [],
      "source": [
        "\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "968wPvdkuD_0"
      },
      "outputs": [],
      "source": [
        "\n",
        "export_dir = \"mnist_curated_idk\"\n",
        "\n",
        "idk_dataset.export(\n",
        "    export_dir=export_dir,\n",
        "    dataset_type=fo.types.ImageClassificationDirectoryTree,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "od-kxyWgu6EA"
      },
      "outputs": [],
      "source": [
        "\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "api = HfApi()\n",
        "repo_id = \"ishalijadhav/mnist-curated\"\n",
        "\n",
        "# Repo already exists, so skip creation\n",
        "print(\"Repo already exists — skipping create_repo()\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cQxO07q1LUB"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import upload_large_folder\n",
        "\n",
        "upload_large_folder(\n",
        "    folder_path=\"mnist_curated_idk\",\n",
        "    repo_id=\"ishalijadhav/mnist-curated\",\n",
        "    repo_type=\"dataset\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMbCl4U72Tss"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}